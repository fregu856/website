+++
#title = "A PhD of Reading: Reflections, Paper Recommendations and Statistics"
#title = "How to Read 300 Papers in 5 Years"
title = "The How and Why of Reading 300 Papers in 5 Years"
#title = "The How and Why of Reading 300 Research Papers in 5 Years"

date = 2023-06-22T00:00:00
lastmod = 2023-06-22T00:00:00
draft = false
math = true

# Authors. Comma separated list, e.g. `["Bob Smith", "David Jones"]`.
authors = []

tags = []
summary = "Since I started my PhD almost five years ago, I have categorized, annotated and written short comments for all research papers I read in detail. I share this publicly in a GitHub repository, and recently reached 300 read papers. To mark this milestone, I decided to share some thoughts on why I think it's important to read a lot of papers, and how I organize my reading. I also compiled some paper statistics, along with a list of 30 papers that I found particularly interesting..."

[header]
image = ""
caption = ""
#image_preview = "1d_regression.png"
image_preview = "phd_of_reading/cover_image_cropped_cropped_cropped.png"
small_image = "phd_of_reading/cover_image_orig.png" # (header image the same size as the article)
#video="https://www.youtube.com/embed/5G4cmSh4s-4?enablejsapi=1&version=3&playerapiid=ytplayer" # (like small_image, but with an embedded youtube video instead)

+++

_The header image above was generated using the text-to-image tool at [imagine.art](https://www.imagine.art/dashboard/tool/from-text), from the input "teddy bear scientists having an intense discussion in a reading group about machine learning"._

Since I started my PhD almost five years ago (in September 2018), I have categorized, annotated and written short comments for all research papers I read in detail. I share this publicly in a [GitHub repository](https://github.com/fregu856/papers), and recently reached 300 read papers. To mark this milestone, I decided to share some thoughts on why I think it's important to read a lot of papers, and how I organize my reading. I also compiled some paper statistics, along with a list of 30 papers that I found particularly interesting and/or well-written.


#### Why I read a lot of papers
The simple reason as to why I try to read a lot of papers, is that I find it very enjoyable and interesting. Among all the different everyday activities of a typical PhD student, reading papers is one of my clear favorites. However, I also think it's key for becoming a good researcher. In fact, I consider reading papers to be a fundamental part of what it even means to be a researcher.

First of all, I have found it to be a very powerful tool for learning new things. It's actually quite remarkable how much you can learn about a certain machine learning problem or method simply by reading e.g. 5 papers about it in detail.

Secondly, I think it's a great way to generate new research ideas. When reading an interesting paper, I virtually always get at least one idea about how the proposed approach potentially could be extended in some way, or how it could be combined with some of my previous work.


#### How I'm able to read a lot of papers
As a PhD student, I'm always quite busy with research, teaching duties, coursework and other meetings. In my experience, the single most important thing one can do in order to read a lot of papers is therefore to _actively prioritize reading_. This means setting aside some time in your schedule each week, specifically for reading. Aiming to read at least one paper a week has generally worked well for me, and I thus make sure to schedule at least one 1-2 hour time slot each week. By treating reading just like any other weekly meeting, I have (almost) always been able to find time for it.

The second most important thing one can do is probably to join (or start) a reading group. I've been organizing a weekly [machine learning reading group](https://www.it.uu.se/about_us/divisions/systems_and_control/activities/mlreadinggroup) at our division since I started as a PhD student. Being a member of a reading group makes it much easier to actually set aside time to read e.g. one paper a week.

Joining a reading group also comes with many other benefits. First of all, it's a good way to "force" yourself to read papers you wouldn't necessarily have selected yourself (such papers can be surprisingly interesting). I think this is important, to not just read the very latest state-of-the-art papers within your specific area of interest. Instead, I think one should actively try to branch out a little and also read some older papers, and papers from other areas.

Moreover, discussing a paper you've read in a reading group almost always significantly improves your understanding of the paper. Joining a reading group is also a great way to learn more about your colleagues’ research and interests, and of course, discussing papers with other people is a generally fun and enjoyable activity in itself.


#### How I organize my paper reading
For each paper I read in detail, I create a note somewhere where I can easily find it later. In this note, I write down any questions, thoughts or ideas which arise during reading. Reading papers can generate a lot of research ideas, and they all seem obvious right there and then, but if you don’t write them down they will be very difficult to remember in the future. A PhD is long, you might want to go back to a certain read paper years later, and then such notes (even if they are very brief) can be incredibly useful.

Afterwards, I also write a very short summary of the paper, quickly answering questions such as "Was the paper interesting overall?", "Was it easy to understand?" or "Could it be relevant for my research now or in the future?". Having short answers to simple questions like these can also turn out to be surprisingly helpful.

For each paper, I also annotate the pdf and upload it to my [GitHub repository](https://github.com/fregu856/papers/tree/master/commented_pdfs) (i.e., somewhere where I can easily find it later). I strongly recommend getting a tablet for reading and annotating papers, it really does make a big difference.

The main reason for why I annotate each paper is that this makes it easier to stay focused while reading. I find that actively annotating a paper "forces" you to actually try to understand what you're reading. Moreover, it enables you to go back to a paper years later and quickly find the most interesting and important information.


#### Why I share my reading publicly
The reason why I share a list of all my read papers publicly on [GitHub](https://github.com/fregu856/papers) is somewhat of a coincidence, it's mostly just an idea I happened to have one day when I started my PhD. Now almost five years later, it is however something that I definitely can recommend.

There are of course much more advanced tools such as [Zotero](https://www.zotero.org/) (which I'm sure can be super useful and convenient), but I really enjoy the simplicity of my "GitHub repository" system, and it seems to provide all the features I need. For example, it enables me to go back and very quickly find particular papers that I've read before.

Moreover, I strongly believe in the general principles of open science. Open exchange and publication of information are defining features of the scientific community, and publicly sharing all of my reading in this way is, at least to me, a natural next step towards truly transparent and accessible research.

Last but not least, publicly sharing my reading definitely motivates me to read more papers. It adds a bit of external pressure and encouragement to stay consistent with my reading, to actually set aside some time (almost) every week. Without it, I'm quite confident that I wouldn't have been able to reach 300 read papers already.


#### How I find interesting papers to read
To read a lot of papers, one also has to find a lot of interesting papers. I regularly look for interesting new and old papers, for example by checking:

- arXiv.
- Twitter.
- Accepted papers lists for upcoming and previous conferences.
- The references in interesting papers.
- The "Cited by" list on Google Scholar for interesting papers.
- A list of people whose research I find particularly interesting, occasionally checking arXiv/scholar for their new papers.

I have a (nearly) daily habit of quickly going through all new [cs.LG](https://arxiv.org/list/cs.LG/recent) and [cs.CV](https://arxiv.org/list/cs.CV/recent) papers on arXiv, and spending 5 minutes scrolling through twitter. The number of papers on arXiv keeps increasing every year, but I still find it manageable and worthwhile to go through them daily (typically, this takes at most 20-30 min). There are many aspects of twitter that I really don't like (thus I limit myself to 5 min per day), but I must admit that it's a good tool for finding interesting papers, researchers, workshops, seminars etc.

Whenever I find a seemingly interesting paper, I go through it very quickly (check the abstract, scroll through the method and/or experiments) and decide whether or not it still seems interesting. If so, I then save it somewhere where I easily can go back and find it later. Specifically, I put the paper titles in Google Keep lists, and if a paper seems especially interesting I also make a short note about that. Once I have time to read a paper in detail, I then go through my lists of interesting papers and pick one.


### 30 Particularly Interesting and/or Well-Written Papers
I went through my comments for all 300 read papers, and selected a list of 30 papers that I found particularly interesting and/or well-written:

**Uncertainty Estimation:**

- [_Weight Uncertainty in Neural Networks_](https://arxiv.org/abs/1505.05424) (ICML 2015)
- [_Cyclical Stochastic Gradient MCMC for Bayesian Deep Learning_](https://arxiv.org/abs/1902.03932) (ICLR 2020)
- [_Laplace Redux -- Effortless Bayesian Deep Learning_](https://arxiv.org/abs/2106.14806) (NeurIPS 2021)
- [_Uncertainty Estimates and Multi-Hypotheses Networks for Optical Flow_](https://arxiv.org/abs/1802.07095) (ECCV 2018)
- [_Deep Reinforcement Learning in a Handful of Trials using Probabilistic Dynamics Models_](https://arxiv.org/abs/1805.12114) (NeurIPS 2018)
- [_Selective Classification for Deep Neural Networks_](https://dl.acm.org/doi/pdf/10.5555/3295222.3295241) (NeurIPS 2017)
- [_Conformalized Quantile Regression_](https://proceedings.neurips.cc/paper/2019/file/5103c3584b063c431bd1268e9b5e76fb-Paper.pdf) (NeurIPS 2019)
- [_On the Practicality of Deterministic Epistemic Uncertainty_](https://arxiv.org/abs/2107.00649) (ICML 2022)

**Out-of-Distribution Detection:**

- [_Hierarchical VAEs Know What They Don't Know_](https://arxiv.org/abs/2102.08248) (ICML 2021)
- [_A Simple Unified Framework for Detecting Out-of-Distribution Samples and Adversarial Attacks_](https://arxiv.org/abs/1807.03888) (NeurIPS 2018)
- [_SSD: A Unified Framework for Self-Supervised Outlier Detection_](https://arxiv.org/abs/2103.12051) (ICLR 2021)
- [_Out-of-Distribution Detection with Deep Nearest Neighbors_](https://arxiv.org/abs/2204.06507) (ICML 2022)
- [_Being a Bit Frequentist Improves Bayesian Neural Networks_](https://arxiv.org/abs/2106.10065) (AISTATS 2022)
- [_Reliable and Trustworthy Machine Learning for Health Using Dataset Shift Detection_](https://arxiv.org/abs/2110.14019) (NeurIPS 2021)
- [_Does Your Dermatology Classifier Know What It Doesn't Know? Detecting the Long-Tail of Unseen Conditions_](https://www.sciencedirect.com/science/article/abs/pii/S1361841521003194?via%3Dihub) (Medical Image Analysis, 2022)

**Energy-Based Models:**

- [_Estimation of Non-Normalized Statistical Models by Score Matching_](http://www.jmlr.org/papers/v6/hyvarinen05a.html) (JMLR, 2005)
- [_Noise-Contrastive Estimation: A New Estimation Principle for Unnormalized Statistical Models_](http://proceedings.mlr.press/v9/gutmann10a/gutmann10a.pdf) (AISTATS 2010)
- [_On the Anatomy of MCMC-Based Maximum Likelihood Learning of Energy-Based Models_](https://arxiv.org/abs/1903.12370) (AAAI 2020)
- [_Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One_](https://arxiv.org/abs/1912.03263) (ICLR 2020)
- [_Flow Contrastive Estimation of Energy-Based Models_](https://arxiv.org/abs/1912.00589) (CVPR 2020)
- [_Joint Training of Variational Auto-Encoder and Latent Energy-Based Model_](https://arxiv.org/abs/2006.06059) (CVPR 2020)

**Other:**

- [_NeRF: Representing Scenes as Neural Radiance Fields for View Synthesis_](https://arxiv.org/abs/2003.08934) (ECCV 2020)
- [_Neural Unsigned Distance Fields for Implicit Function Learning_](https://arxiv.org/abs/2010.13938) (NeurIPS 2020)
- [_Expressive Body Capture: 3D Hands, Face, and Body from a Single Image_](https://arxiv.org/abs/1904.05866) (CVPR 2019)
- [_A Baseline for 3D Multi-Object Tracking_](https://arxiv.org/abs/1907.03961) (IROS 2020)
- [_Learning Latent Dynamics for Planning from Pixels_](https://arxiv.org/abs/1811.04551) (ICML 2019)
- [_Blind Spots in AI Ethics_](https://link.springer.com/article/10.1007/s43681-021-00122-8) (AI and Ethics, 2022)
- [_The Uselessness of AI Ethics_](https://link.springer.com/article/10.1007/s43681-022-00209-w) (AI and Ethics, 2022)
- [_Why AI is Harder Than We Think_](https://arxiv.org/abs/2104.12871) (GECCO 2021)
- [_Talking About Large Language Models_](https://arxiv.org/abs/2212.03551) (arXiv, 2022)


### Paper Statistics

I compiled some more detailed statistics of my read papers (number of papers read each year, papers by publication year, papers by venue, papers by category):

303 read papers.

1.84 GB of annotated pdfs.

![Papers read each year](/img/phd_of_reading/papers_read_each_year.svg)
![Papers by publication year](/img/phd_of_reading/papers_by_publication_year.svg)
![Papers by venue](/img/phd_of_reading/papers_by_venue.svg)
![Papers by category](/img/phd_of_reading/papers_by_category.svg)
